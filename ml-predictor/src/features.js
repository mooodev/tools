/**
 * Feature Engineering Pipeline (Simons / Renaissance Style)
 *
 * Converts raw OHLCV candles into a rich feature matrix suitable for ML.
 * Features include:
 *   - Multi-scale TA indicators
 *   - Normalized price ratios
 *   - Statistical moments (volatility, skew, kurtosis)
 *   - Regime detection (Hurst exponent, volatility regime)
 *   - Volume profile features
 *   - Cross-feature interactions
 *   - Lagged returns at multiple horizons
 */

const config = require("./config");
const ind = require("./indicators");

// Feature names generated by computeFeatures (set on first call)
let _featureNames = null;

/**
 * Compute all features for a single token's candle array.
 * Returns { features: number[][], labels: number[], featureNames: string[] }
 *
 * features[i] = feature vector for candle i (only where all indicators are valid)
 * labels[i]   = classification label for candle i
 *   0 = bearish (return < GROWTH_THRESHOLD_DN)
 *   1 = neutral
 *   2 = bullish (return > GROWTH_THRESHOLD_UP)
 */
function computeFeatures(candles) {
  const n = candles.length;
  const opens = candles.map((c) => c.open);
  const highs = candles.map((c) => c.high);
  const lows = candles.map((c) => c.low);
  const closes = candles.map((c) => c.close);
  const volumes = candles.map((c) => c.volume);

  // ─── Compute all indicators ──────────────────────────────────────

  const featureColumns = {};

  // Log returns
  const logReturns = new Array(n).fill(NaN);
  for (let i = 1; i < n; i++) {
    logReturns[i] = closes[i - 1] !== 0 ? Math.log(closes[i] / closes[i - 1]) : 0;
  }
  featureColumns["log_return"] = logReturns;

  // Multi-scale ROC (rate of change)
  for (const p of config.ROC_PERIODS) {
    featureColumns[`roc_${p}`] = ind.roc(closes, p);
  }

  // SMA ratios (price / SMA - captures mean reversion)
  for (const p of config.SMA_PERIODS) {
    const smaVals = ind.sma(closes, p);
    featureColumns[`sma_ratio_${p}`] = closes.map((c, i) =>
      !isNaN(smaVals[i]) && smaVals[i] !== 0 ? c / smaVals[i] - 1 : NaN
    );
  }

  // EMA ratios
  for (const p of config.EMA_PERIODS) {
    const emaVals = ind.ema(closes, p);
    featureColumns[`ema_ratio_${p}`] = closes.map((c, i) =>
      !isNaN(emaVals[i]) && emaVals[i] !== 0 ? c / emaVals[i] - 1 : NaN
    );
  }

  // EMA crossover signals (fast/slow ratios)
  const ema12 = ind.ema(closes, 12);
  const ema26 = ind.ema(closes, 26);
  featureColumns["ema_cross_12_26"] = ema12.map((v, i) =>
    !isNaN(v) && !isNaN(ema26[i]) && ema26[i] !== 0 ? v / ema26[i] - 1 : NaN
  );

  // RSI (normalized to 0-1)
  const rsiVals = ind.rsi(closes, config.RSI_PERIOD);
  featureColumns["rsi"] = rsiVals.map((v) => (!isNaN(v) ? v / 100 : NaN));

  // MACD features
  const macdResult = ind.macd(closes, config.MACD_FAST, config.MACD_SLOW, config.MACD_SIGNAL);
  // Normalize MACD by price to make it comparable across tokens
  featureColumns["macd_norm"] = macdResult.macdLine.map((v, i) =>
    !isNaN(v) && closes[i] !== 0 ? v / closes[i] : NaN
  );
  featureColumns["macd_signal_norm"] = macdResult.signal.map((v, i) =>
    !isNaN(v) && closes[i] !== 0 ? v / closes[i] : NaN
  );
  featureColumns["macd_hist_norm"] = macdResult.histogram.map((v, i) =>
    !isNaN(v) && closes[i] !== 0 ? v / closes[i] : NaN
  );

  // Bollinger Bands
  const bb = ind.bollingerBands(closes, config.BBANDS_PERIOD, config.BBANDS_STD);
  featureColumns["bb_percent_b"] = bb.percentB;
  featureColumns["bb_bandwidth"] = bb.bandwidth;

  // ATR (normalized by price)
  const atrResult = ind.atr(highs, lows, closes, config.ATR_PERIOD);
  featureColumns["atr_norm"] = atrResult.atr.map((v, i) =>
    !isNaN(v) && closes[i] !== 0 ? v / closes[i] : NaN
  );

  // Stochastic Oscillator (normalized to 0-1)
  const stoch = ind.stochastic(highs, lows, closes, config.STOCH_K_PERIOD, config.STOCH_D_PERIOD);
  featureColumns["stoch_k"] = stoch.k.map((v) => (!isNaN(v) ? v / 100 : NaN));
  featureColumns["stoch_d"] = stoch.d.map((v) => (!isNaN(v) ? v / 100 : NaN));

  // CCI (normalized by dividing by 200 to roughly bound to [-1, 1])
  const cciVals = ind.cci(highs, lows, closes, config.CCI_PERIOD);
  featureColumns["cci_norm"] = cciVals.map((v) => (!isNaN(v) ? v / 200 : NaN));

  // Williams %R (normalized to 0-1)
  const willR = ind.williamsR(highs, lows, closes, config.WILLIAMS_PERIOD);
  featureColumns["williams_r"] = willR.map((v) => (!isNaN(v) ? (v + 100) / 100 : NaN));

  // OBV (normalized: z-score style using rolling mean/std)
  if (config.OBV_ENABLED) {
    const obvVals = ind.obv(closes, volumes);
    const obvSma = ind.sma(obvVals, 20);
    featureColumns["obv_ratio"] = obvVals.map((v, i) =>
      !isNaN(obvSma[i]) && obvSma[i] !== 0 ? v / Math.abs(obvSma[i]) - 1 : NaN
    );
  }

  // VWAP ratio
  if (config.VWAP_ENABLED) {
    const vwapVals = ind.vwap(highs, lows, closes, volumes);
    featureColumns["vwap_ratio"] = closes.map((c, i) =>
      !isNaN(vwapVals[i]) && vwapVals[i] !== 0 ? c / vwapVals[i] - 1 : NaN
    );
  }

  // MFI (normalized to 0-1)
  const mfiVals = ind.mfi(highs, lows, closes, volumes, 14);
  featureColumns["mfi"] = mfiVals.map((v) => (!isNaN(v) ? v / 100 : NaN));

  // ─── Statistical / Simons-style features ─────────────────────────

  // Multi-scale volatility
  for (const p of [10, 20, 40]) {
    featureColumns[`volatility_${p}`] = ind.rollingVolatility(closes, p);
  }

  // Volatility ratio (short/long) — regime change detector
  const vol10 = ind.rollingVolatility(closes, 10);
  const vol40 = ind.rollingVolatility(closes, 40);
  featureColumns["vol_ratio_10_40"] = vol10.map((v, i) =>
    !isNaN(v) && !isNaN(vol40[i]) && vol40[i] !== 0 ? v / vol40[i] : NaN
  );

  // Skewness & Kurtosis (distributional features)
  featureColumns["skewness_20"] = ind.rollingSkewness(closes, 20);
  featureColumns["kurtosis_20"] = ind.rollingKurtosis(closes, 20);

  // Hurst exponent (mean-reversion vs trend detection)
  featureColumns["hurst"] = ind.hurstExponent(closes, 40);

  // ─── Price structure features ────────────────────────────────────

  // Candle body ratio (captures candle patterns)
  featureColumns["body_ratio"] = candles.map((c) => {
    const range = c.high - c.low;
    return range !== 0 ? (c.close - c.open) / range : 0;
  });

  // Upper/lower shadow ratios
  featureColumns["upper_shadow"] = candles.map((c) => {
    const range = c.high - c.low;
    const body_top = Math.max(c.open, c.close);
    return range !== 0 ? (c.high - body_top) / range : 0;
  });

  featureColumns["lower_shadow"] = candles.map((c) => {
    const range = c.high - c.low;
    const body_bottom = Math.min(c.open, c.close);
    return range !== 0 ? (body_bottom - c.low) / range : 0;
  });

  // Volume features
  const volSma20 = ind.sma(volumes, 20);
  featureColumns["vol_sma_ratio"] = volumes.map((v, i) =>
    !isNaN(volSma20[i]) && volSma20[i] !== 0 ? v / volSma20[i] - 1 : NaN
  );

  // Volume-price correlation (rolling)
  featureColumns["vol_price_corr"] = rollingCorrelation(logReturns, volumes, 20);

  // ─── Time features (cyclical encoding) ───────────────────────────

  featureColumns["hour_sin"] = candles.map((c) => {
    const h = new Date(c.timestamp * 1000).getUTCHours();
    return Math.sin((2 * Math.PI * h) / 24);
  });
  featureColumns["hour_cos"] = candles.map((c) => {
    const h = new Date(c.timestamp * 1000).getUTCHours();
    return Math.cos((2 * Math.PI * h) / 24);
  });
  featureColumns["dow_sin"] = candles.map((c) => {
    const d = new Date(c.timestamp * 1000).getUTCDay();
    return Math.sin((2 * Math.PI * d) / 7);
  });
  featureColumns["dow_cos"] = candles.map((c) => {
    const d = new Date(c.timestamp * 1000).getUTCDay();
    return Math.cos((2 * Math.PI * d) / 7);
  });

  // ─── Assemble feature matrix ─────────────────────────────────────

  const featureNames = Object.keys(featureColumns);
  _featureNames = featureNames;

  // Compute labels: future return classification
  const labels = new Array(n).fill(NaN);
  const horizon = config.PREDICTION_HORIZON;
  for (let i = 0; i < n - horizon; i++) {
    if (closes[i] === 0) continue;
    const futureReturn = (closes[i + horizon] - closes[i]) / closes[i];
    if (futureReturn > config.GROWTH_THRESHOLD_UP) {
      labels[i] = 2; // Bullish
    } else if (futureReturn < config.GROWTH_THRESHOLD_DN) {
      labels[i] = 0; // Bearish
    } else {
      labels[i] = 1; // Neutral
    }
  }

  // Find valid start index (where all features are non-NaN)
  const warmup = Math.max(
    config.SMA_PERIODS[config.SMA_PERIODS.length - 1],
    config.EMA_PERIODS[config.EMA_PERIODS.length - 1],
    config.RSI_PERIOD + 1,
    config.MACD_SLOW + config.MACD_SIGNAL,
    config.BBANDS_PERIOD,
    config.ATR_PERIOD,
    config.STOCH_K_PERIOD + config.STOCH_D_PERIOD,
    config.CCI_PERIOD,
    config.WILLIAMS_PERIOD,
    41, // Hurst exponent window
    20, // Volume SMA
    20, // Correlation window
  );

  const features = [];
  const validLabels = [];

  for (let i = warmup; i < n - horizon; i++) {
    const row = [];
    let valid = true;

    for (const name of featureNames) {
      const val = featureColumns[name][i];
      if (isNaN(val) || val === undefined || val === null) {
        valid = false;
        break;
      }
      row.push(val);
    }

    if (!valid || isNaN(labels[i])) continue;

    features.push(row);
    validLabels.push(labels[i]);
  }

  return { features, labels: validLabels, featureNames };
}

/**
 * Rolling Pearson correlation between two arrays.
 */
function rollingCorrelation(a, b, period) {
  const n = a.length;
  const result = new Array(n).fill(NaN);

  for (let i = period; i < n; i++) {
    let sumA = 0, sumB = 0, sumAB = 0, sumA2 = 0, sumB2 = 0;

    for (let j = i - period + 1; j <= i; j++) {
      const va = isNaN(a[j]) ? 0 : a[j];
      const vb = isNaN(b[j]) ? 0 : b[j];
      sumA += va;
      sumB += vb;
      sumAB += va * vb;
      sumA2 += va * va;
      sumB2 += vb * vb;
    }

    const num = period * sumAB - sumA * sumB;
    const den = Math.sqrt(
      (period * sumA2 - sumA * sumA) * (period * sumB2 - sumB * sumB)
    );

    result[i] = den !== 0 ? num / den : 0;
  }

  return result;
}

/**
 * Get the feature names from the last computeFeatures call.
 */
function getFeatureNames() {
  return _featureNames;
}

/**
 * Create sliding windows for LSTM input.
 * Takes flat feature matrix and creates [samples, timesteps, features] shape.
 *
 * @param {number[][]} features - Feature matrix [n_samples x n_features]
 * @param {number[]} labels - Labels array [n_samples]
 * @param {number} windowSize - LSTM lookback window
 * @returns {{ X: number[][][], y: number[] }}
 */
function createSequences(features, labels, windowSize) {
  const X = [];
  const y = [];

  for (let i = windowSize; i < features.length; i++) {
    X.push(features.slice(i - windowSize, i));
    y.push(labels[i]);
  }

  return { X, y };
}

/**
 * Normalize features using z-score normalization.
 * Returns normalized data and the scaler parameters for inference.
 *
 * @param {number[][]} features - Feature matrix
 * @returns {{ normalized: number[][], scaler: { means: number[], stds: number[] } }}
 */
function normalizeFeatures(features) {
  if (features.length === 0) return { normalized: [], scaler: { means: [], stds: [] } };

  const nFeatures = features[0].length;
  const means = new Array(nFeatures).fill(0);
  const stds = new Array(nFeatures).fill(0);

  // Compute means
  for (const row of features) {
    for (let j = 0; j < nFeatures; j++) {
      means[j] += row[j];
    }
  }
  for (let j = 0; j < nFeatures; j++) means[j] /= features.length;

  // Compute standard deviations
  for (const row of features) {
    for (let j = 0; j < nFeatures; j++) {
      stds[j] += Math.pow(row[j] - means[j], 2);
    }
  }
  for (let j = 0; j < nFeatures; j++) {
    stds[j] = Math.sqrt(stds[j] / features.length);
    if (stds[j] === 0) stds[j] = 1; // Avoid division by zero
  }

  // Normalize
  const normalized = features.map((row) =>
    row.map((val, j) => (val - means[j]) / stds[j])
  );

  // Clip extreme values to [-5, 5] to handle outliers (Simons approach)
  for (const row of normalized) {
    for (let j = 0; j < row.length; j++) {
      row[j] = Math.max(-5, Math.min(5, row[j]));
    }
  }

  return { normalized, scaler: { means, stds } };
}

/**
 * Apply a pre-fitted scaler to new data.
 */
function applyScaler(features, scaler) {
  return features.map((row) =>
    row.map((val, j) => {
      const z = (val - scaler.means[j]) / scaler.stds[j];
      return Math.max(-5, Math.min(5, z));
    })
  );
}

module.exports = {
  computeFeatures,
  createSequences,
  normalizeFeatures,
  applyScaler,
  getFeatureNames,
};
